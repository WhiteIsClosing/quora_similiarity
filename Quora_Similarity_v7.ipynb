{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Load Required Python Libraries\n",
    "##########################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pylev import levenshtein\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import chardet\n",
    "import itertools\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from scipy.stats import kurtosis\n",
    "from sklearn.externals import joblib\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora, models\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Dataset\n",
    "##########################################\n",
    "#Training Dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "data['question1'] = data['question1'].astype(str)\n",
    "data['question2'] = data['question2'].astype(str)\n",
    "y = data['is_duplicate']\n",
    "df_train = data\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Test Dataset\n",
    "##########################################\n",
    "#Test Dataset\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#Replaces np.nan with ''\n",
    "df_test = df_test.replace(np.nan, '', regex=True)\n",
    "\n",
    "#Saves the cleaned test.csv\n",
    "# df_test.to_csv('cleaned_test.csv')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Initializes variables for Feature Creation\n",
    "##########################################\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "model = KeyedVectors.load(\"300features_10minwords_5context\")\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Function for Magic Features\n",
    "##########################################\n",
    "def try_apply_dict(x,dict_to_apply):\n",
    "    try:\n",
    "        return dict_to_apply[x]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def magic_features(data = df_train, test_data = df_test):\n",
    "    df1 = data[['question1']].copy()\n",
    "    df2 = data[['question2']].copy()\n",
    "    df1_test = test_data[['question1']].copy()\n",
    "    df2_test = test_data[['question2']].copy()\n",
    "\n",
    "    df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "    df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "    train_questions = df1.append(df2)\n",
    "    train_questions = train_questions.append(df1_test)\n",
    "    train_questions = train_questions.append(df2_test)\n",
    "    train_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
    "\n",
    "    train_questions.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    questions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
    "\n",
    "    train_cp = data.copy()\n",
    "    test_cp = test_data.copy()\n",
    "    train_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n",
    "\n",
    "    test_cp['is_duplicate'] = -1\n",
    "    test_cp.rename(columns={'test_id':'id'},inplace=True)\n",
    "\n",
    "    comb = pd.concat([train_cp,test_cp])\n",
    "\n",
    "    comb['q1_hash'] = comb['question1'].map(questions_dict)\n",
    "    comb['q2_hash'] = comb['question2'].map(questions_dict)\n",
    "\n",
    "    q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "    q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "    #map to frequency space\n",
    "    comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "    comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "\n",
    "    train_comb = comb[comb['is_duplicate'] >= 0][['q1_hash','q2_hash','q1_freq','q2_freq']]\n",
    "    test_comb = comb[comb['is_duplicate'] < 0][['q1_hash','q2_hash','q1_freq','q2_freq']]\n",
    "    return np.array(train_comb), np.array(test_comb)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Function and Transformer for Word2Vec Features\n",
    "##########################################\n",
    "def question_to_wordlist(text, remove_stopwords = False):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "    return(words)\n",
    "\n",
    "def makeDistributionalFeatures(q1,q2):\n",
    "    data = pd.concat([q1, q2], axis=1)\n",
    "    features = []\n",
    "    \n",
    "    #For each question in the dataset:\n",
    "    # 1 - Compute similarity metric from word2vec model using every word combination between question1 and question2\n",
    "    # 2 - Create the distributional summary statistics for every combination of the disimilar words\n",
    "    for index in range(0, len(data)):\n",
    "            #Convert question1 and question2 into a list of words\n",
    "            question1 = question_to_wordlist(data.question1[index])\n",
    "            question2 = question_to_wordlist(data.question2[index])\n",
    "            \n",
    "            #Finds every word combination between question1 and question2\n",
    "            combinations = list(itertools.product(question1, question2))\n",
    "            combinations = [list(combination) for combination in combinations]\n",
    "            \n",
    "            #Tracks word2vec similarity metric for every word combination\n",
    "            values = []\n",
    "            \n",
    "            #Loops through each word combination\n",
    "            for combination in combinations:\n",
    "                #Checks if the model contains the words in its vocabulary\n",
    "                # 1 - Yes, adds it to the values list to calculate distributional stats with\n",
    "                # 2 - No, go to the next word pair\n",
    "                try:\n",
    "                    values.append(model.wv.similarity(combination[0], combination[1]))\n",
    "                except KeyError:\n",
    "                    pass\n",
    "      \n",
    "            #If there is at least one similarity metric calculate its mean and median\n",
    "            if(len(values) >= 1):\n",
    "                features.append([np.mean(values), np.median(values), np.std(values), kurtosis(values)])\n",
    "            else:\n",
    "                #Since we will not be deleting observations from the test dataset append [-1,-1,-1,-1] as stand in features\n",
    "                # 1 - The only combination contained a word our model does not contain\n",
    "                # 2 - Question1 or Question2 or both were \"\"\n",
    "                # 3 - We could not tokenize either Question1 or Question2\n",
    "                features.append([-1,-1,-1,-1])\n",
    "    return features\n",
    "\n",
    "class Word2VecStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the generates the mean/median/std/kurtosis between each string, returns array of lists\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        stats = makeDistributionalFeatures(q1_list, q2_list)\n",
    "        return np.array(stats)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self  \n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Function and Transformer for Average Shared Words\n",
    "##########################################\n",
    "def shared_words(q1,q2):\n",
    "    question1_words = []\n",
    "    question2_words = []\n",
    "\n",
    "    for word in set(q1.lower().split()):\n",
    "        if word not in stop_words:\n",
    "            question1_words.append(word)\n",
    "\n",
    "    for word in set(q2.lower().split()):\n",
    "        if word not in stop_words:\n",
    "            question2_words.append(word)\n",
    "\n",
    "    #Question contains only stop words (or is an empty string)\n",
    "    if len(question1_words) == 0 or len(question2_words) == 0:\n",
    "        return 0\n",
    "\n",
    "    question1_shared_words = [w for w in question1_words if w in question2_words]\n",
    "    question2_shared_words = [w for w in question2_words if w in question1_words]\n",
    "\n",
    "    avg_words_shared = (len(question1_shared_words) + len(question2_shared_words))/(len(question1_words) + len(question2_words))\n",
    "    return avg_words_shared\n",
    "\n",
    "class AverageSharedWords(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the average shared words between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        avg_words = [shared_words(q1,q2) for q1, q2 in zip(q1_list, q2_list)]\n",
    "\n",
    "        return np.array(avg_words).reshape(len(avg_words),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Function and Transformer for Word Length\n",
    "##########################################\n",
    "def word_lengths(q1,q2):\n",
    "    data = pd.concat([q1, q2], axis=1)\n",
    "\n",
    "    #Feature: Length of Question\n",
    "    data['len_q1'] = data.question1.apply(lambda x: len(x))\n",
    "    data['len_q2'] = data.question2.apply(lambda x: len(x))\n",
    "\n",
    "    #Feature: Difference in length between the Questions\n",
    "    data['len_diff'] = data.len_q1 - data.len_q2\n",
    "\n",
    "    #Feature: Character count of Question\n",
    "    data['len_char_q1'] = data.question1.apply(lambda x: len(x.replace(' ', '')))\n",
    "    data['len_char_q2'] = data.question2.apply(lambda x: len(x.replace(' ', '')))\n",
    "\n",
    "    #Feature: Word count of Question\n",
    "    data['len_word_q1'] = data.question1.apply(lambda x: len(x.split()))\n",
    "    data['len_word_q2'] = data.question2.apply(lambda x: len(x.split()))\n",
    "\n",
    "    #Feature: Common words between the Questions\n",
    "    data['len_common_words'] = data.apply(lambda x: len(set(x['question1'].lower().split()).intersection(set(x['question2'].lower().split()))), axis=1)\n",
    "    return data.ix[:,'len_q1':'len_common_words']\n",
    "\n",
    "class WordLengths(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the word lengths between each string, returns array of lists\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        word_len = word_lengths(q1_list, q2_list)\n",
    "        return np.array(word_len)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self  \n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Transformers for Levenshtein and Tfidf\n",
    "##########################################\n",
    "class LevDistanceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        \n",
    "        lev_distance_strings = [[a,b] for a,b in zip(q1_list, q2_list)]\n",
    "        \n",
    "        lev_dist_array = np.array([(float(levenshtein(pair[0], pair[1]))/\n",
    "                                    (float(sum([x.count('') for x in pair[0]])) + float(sum([x.count('') for x in pair[1]])))) \n",
    "                                    for pair in lev_distance_strings \n",
    "                                    ])\n",
    "        \n",
    "        return lev_dist_array.reshape(len(lev_dist_array),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class TfIdfDiffTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the tfidf difference between each string, returns tfidf matrix\"\"\"\n",
    "\n",
    "    def __init__(self, total_words):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_questions = list(q1_list) + list(q2_list)\n",
    "        total_questions = [x for x in total_questions if type(x) != float]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words = 'english', vocabulary = total_words)\n",
    "        vectorizer.fit(total_questions)\n",
    "        tf_diff = vectorizer.transform(q1_list) - vectorizer.transform(q2_list)\n",
    "        return tf_diff\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Transformer for LDA\n",
    "##########################################\n",
    "class LDATransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the topics and probability for each string, returns list of lists\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_questions = list(q1_list) + list(q2_list)\n",
    "\n",
    "        #Tokenize each question\n",
    "        questions = [question_to_wordlist(question, remove_stopwords = True) for question in total_questions]\n",
    "\n",
    "        #Create a Gensim dictionary from the questions\n",
    "        dictionary = Dictionary(questions)\n",
    "\n",
    "        #Remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "        dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "        #Convert the dictionary to a Bag of Words corpus for reference\n",
    "        corpus = [dictionary.doc2bow(question) for question in questions]\n",
    "\n",
    "        #Train LDA model\n",
    "        topics=300\n",
    "        lda = models.LdaMulticore(corpus, id2word=dictionary, num_topics=topics, workers=20)\n",
    "\n",
    "        #Return Document Topics for Question1\n",
    "        empty = np.zeros(shape=(len(q1_list),topics*2))\n",
    "        empty[empty == 0] = -1\n",
    "\n",
    "        colNames = []\n",
    "        for i in range(0, topics):\n",
    "            colNames.append('q1_topic' + str(i))\n",
    "            colNames.append('q1_proba' + str(i))\n",
    "\n",
    "        q1_df = pd.DataFrame(empty, columns=colNames)\n",
    "\n",
    "        for x in tqdm(range(0, len(q1_list))):\n",
    "            topic_list = lda.get_document_topics(corpus[x])\n",
    "            for topic in topic_list:\n",
    "                t = topic[0]\n",
    "                p = topic[1]\n",
    "                q1_df['q1_topic'+str(t)][x] = t\n",
    "                q1_df['q1_proba'+str(t)][x] = p\n",
    "\n",
    "        #Return Document Topics for Question2\n",
    "        empty = np.zeros(shape=(len(q2_list),topics*2))\n",
    "        empty[empty == 0] = -1\n",
    "\n",
    "        colNames = []\n",
    "        for i in range(0, topics):\n",
    "            colNames.append('q2_topic' + str(i))\n",
    "            colNames.append('q2_proba' + str(i))\n",
    "\n",
    "        q2_df = pd.DataFrame(empty, columns=colNames)\n",
    "\n",
    "        for x in tqdm(range(len(q1_list), len(corpus))):\n",
    "            topic_list = lda.get_document_topics(corpus[x])\n",
    "            for topic in topic_list:\n",
    "                t = topic[0]\n",
    "                p = topic[1]\n",
    "                q2_df['q2_topic'+str(t)][x-len(q1_list)] = t\n",
    "                q2_df['q2_proba'+str(t)][x-len(q1_list)] = p\n",
    "        \n",
    "        total_df = pd.concat([q1_df, q2_df], axis=1)\n",
    "        return total_df\n",
    "    \n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use word vocabulary from training data\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizer.fit(df_train['question1'] + df_train['question2'])\n",
    "total_words = list(set(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Combining all the features using FeatureUnion\n",
    "##########################################\n",
    "#Create Magic Features\n",
    "magic_train, magic_test = magic_features()\n",
    "\n",
    "#Feature Union Features\n",
    "comb_features = FeatureUnion([('tf', TfIdfDiffTransformer(total_words)), \n",
    "                              ('lev', LevDistanceTransformer()),\n",
    "                              ('AvgWords', AverageSharedWords()),\n",
    "                              ('WordLengths', WordLengths()),\n",
    "                              ('Word2VecStats', Word2VecStats())\n",
    "                             ])\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Create features using FeatureUnion and Magic Features\n",
    "##########################################\n",
    "y = df_train.ix[:,'is_duplicate']\n",
    "all_features = comb_features.transform([df_train['question1'], df_train['question2']])\n",
    "\n",
    "#Merge FeatureUnion features with Magic Features\n",
    "total_features = scipy.sparse.hstack(blocks=[all_features, magic_train])\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<404290x86721 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 9882154 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total_features.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saves Training Features\n",
    "joblib.dump(total_features, 'total_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Split the dataset into training and testing datasets\n",
    "##########################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(total_features, y, test_size=0.2, random_state=1317)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.683926\ttest-logloss:0.684002\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.608543\ttest-logloss:0.609292\n",
      "[20]\ttrain-logloss:0.554786\ttest-logloss:0.556198\n",
      "[30]\ttrain-logloss:0.514818\ttest-logloss:0.516899\n",
      "[40]\ttrain-logloss:0.484263\ttest-logloss:0.486984\n",
      "[50]\ttrain-logloss:0.460863\ttest-logloss:0.464182\n",
      "[60]\ttrain-logloss:0.442677\ttest-logloss:0.446556\n",
      "[70]\ttrain-logloss:0.428061\ttest-logloss:0.432454\n",
      "[80]\ttrain-logloss:0.416287\ttest-logloss:0.421208\n",
      "[90]\ttrain-logloss:0.406758\ttest-logloss:0.412195\n",
      "[100]\ttrain-logloss:0.398841\ttest-logloss:0.404739\n",
      "[110]\ttrain-logloss:0.392199\ttest-logloss:0.398551\n",
      "[120]\ttrain-logloss:0.386552\ttest-logloss:0.393368\n",
      "[130]\ttrain-logloss:0.381867\ttest-logloss:0.389113\n",
      "[140]\ttrain-logloss:0.37781\ttest-logloss:0.385504\n",
      "[150]\ttrain-logloss:0.374169\ttest-logloss:0.382276\n",
      "[160]\ttrain-logloss:0.371221\ttest-logloss:0.379684\n",
      "[170]\ttrain-logloss:0.368734\ttest-logloss:0.377513\n",
      "[180]\ttrain-logloss:0.366509\ttest-logloss:0.37561\n",
      "[190]\ttrain-logloss:0.364606\ttest-logloss:0.373966\n",
      "[200]\ttrain-logloss:0.362599\ttest-logloss:0.372231\n",
      "[210]\ttrain-logloss:0.361098\ttest-logloss:0.370936\n",
      "[220]\ttrain-logloss:0.359646\ttest-logloss:0.369732\n",
      "[230]\ttrain-logloss:0.358413\ttest-logloss:0.368721\n",
      "[240]\ttrain-logloss:0.357355\ttest-logloss:0.367858\n",
      "[250]\ttrain-logloss:0.356384\ttest-logloss:0.367088\n",
      "[260]\ttrain-logloss:0.355422\ttest-logloss:0.366348\n",
      "[270]\ttrain-logloss:0.354339\ttest-logloss:0.365501\n",
      "[280]\ttrain-logloss:0.353525\ttest-logloss:0.364887\n",
      "[290]\ttrain-logloss:0.352787\ttest-logloss:0.364327\n",
      "[300]\ttrain-logloss:0.352064\ttest-logloss:0.363757\n",
      "[310]\ttrain-logloss:0.351437\ttest-logloss:0.363272\n",
      "[320]\ttrain-logloss:0.350558\ttest-logloss:0.362585\n",
      "[330]\ttrain-logloss:0.349745\ttest-logloss:0.361984\n",
      "[340]\ttrain-logloss:0.349081\ttest-logloss:0.361468\n",
      "[350]\ttrain-logloss:0.348392\ttest-logloss:0.36096\n",
      "[360]\ttrain-logloss:0.347842\ttest-logloss:0.360547\n",
      "[370]\ttrain-logloss:0.347329\ttest-logloss:0.360191\n",
      "[380]\ttrain-logloss:0.346825\ttest-logloss:0.359829\n",
      "[390]\ttrain-logloss:0.346311\ttest-logloss:0.359444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgboost_model_400iterations_8depth.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################\n",
    "# Running XGBoost\n",
    "##########################################\n",
    "# Set parameters for XGBoost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 8\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'test')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=50, verbose_eval=10)\n",
    "joblib.dump(bst, 'xgboost_model_1000iterations_8depth.pkl')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_features.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################\n",
    "# Create the test features using FeatureUnion\n",
    "##########################################\n",
    "test_features = comb_features.transform([df_test['question1'], df_test['question2']])\n",
    "\n",
    "#Merge FeatureUnion features with Magic Features\n",
    "total_test_features = scipy.sparse.hstack(blocks=[test_features, magic_test])\n",
    "\n",
    "joblib.dump(total_test_features, 'test_features.pkl')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2345796x86721 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 59340301 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Predicting using XGBoost\n",
    "##########################################\n",
    "test = xgb.DMatrix(total_test_features)\n",
    "test_prediction = bst.predict(test)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Creating Submission File\n",
    "##########################################\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = test_prediction\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2345796"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Submission File Length\n",
    "len(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['submission.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(sub, 'submission.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
