{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Load Required Python Libraries\n",
    "##########################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pylev import levenshtein\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import chardet\n",
    "import itertools\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Dataset\n",
    "##########################################\n",
    "#Training Dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "data['question1'] = data['question1'].astype(str)\n",
    "data['question2'] = data['question2'].astype(str)\n",
    "y = data['is_duplicate']\n",
    "#Drop irrelevant features\n",
    "#data = data.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "df_train = data\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#These are the functions for our transformers\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def shared_words(q1,q2):\n",
    "    question1_words = []\n",
    "    question2_words = []\n",
    "\n",
    "    for word in set(str(q1).lower().split()):\n",
    "        if word not in stop_words:\n",
    "            question1_words.append(word)\n",
    "\n",
    "    for word in set(str(q2).lower().split()):\n",
    "        if word not in stop_words:\n",
    "            question2_words.append(word)\n",
    "\n",
    "    #Question contains only stop words (or is an empty string)\n",
    "    if len(question1_words) == 0 or len(question2_words) == 0:\n",
    "        return 0\n",
    "\n",
    "    question1_shared_words = [w for w in question1_words if w in question2_words]\n",
    "    question2_shared_words = [w for w in question2_words if w in question1_words]\n",
    "\n",
    "    avg_words_shared = (len(question1_shared_words) + len(question2_shared_words))/(len(question1_words) + len(question2_words))\n",
    "    return avg_words_shared\n",
    "\n",
    "class LevDistanceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        \n",
    "        lev_distance_strings = [[a,b] \n",
    "        for a,b in zip(q1_list, q2_list)]\n",
    "        \n",
    "        lev_dist_array = np.array([\n",
    "    (float(levenshtein(pair[0], pair[1]))/\n",
    "    (float(sum([x.count('') for x in pair[0]])) + \n",
    "    float(sum([x.count('') for x in pair[1]])))) \n",
    "    for pair in lev_distance_strings \n",
    "        ])\n",
    "        \n",
    "        return lev_dist_array.reshape(len(lev_dist_array),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class TfIdfDiffTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self, total_words):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_questions = q1_list + q2_list\n",
    "        total_questions = [x for x in total_questions if type(x) != float]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words = 'english', vocabulary = total_words)\n",
    "        vectorizer.fit(total_questions)\n",
    "        tf_diff = vectorizer.transform(q1_list) - vectorizer.transform(q2_list)\n",
    "        return tf_diff\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class CosineDistTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_questions = q1_list + q2_list\n",
    "        total_questions = [x for x in total_questions if type(x) != float]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "        vectorizer.fit(total_questions)\n",
    "        \n",
    "        q1_tf = vectorizer.transform(q1_list) \n",
    "        q2_tf = vectorizer.transform(q2_list)\n",
    "        cos_sim = []\n",
    "        for i in range(0,len(q1_list)):\n",
    "            cos_sim.append(cosine_similarity(q1_tf[i], q2_tf[i])[0][0])\n",
    "            \n",
    "        return np.array(cos_sim).reshape(len(cos_sim),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class AverageSharedWords(BaseEstimator, TransformerMixin):\n",
    "\t\"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef transform(self, question_list):\n",
    "\t\tq1_list = question_list[0]\n",
    "\t\tq2_list = question_list[1]\n",
    "\t\tavg_words = [shared_words(q1,q2) for q1, q2 in zip(q1_list, q2_list)]\n",
    "\t\t\t\n",
    "\t\treturn np.array(avg_words).reshape(len(avg_words),1)\n",
    "\n",
    "\tdef fit(self, question_list, y=None):\n",
    "\t\t\"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "\t\treturn self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizer.fit(data['question1'] + data['question2'])\n",
    "total_words = list(set(vectorizer.get_feature_names()))\n",
    "\n",
    "comb_features = FeatureUnion([('tf', TfIdfDiffTransformer(total_words)), \n",
    "                              ('cos_diff',CosineDistTransformer()), \n",
    "                              ('lev', LevDistanceTransformer()),\n",
    "                              ('AvgWords', AverageSharedWords())\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Split the dataset into training and testing datasets\n",
    "##########################################\n",
    "y = data.ix[:,'is_duplicate'][0:5000]\n",
    "all_features = comb_features.transform([data['question1'][0:5000], data['question2'][0:5000]])\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, y, test_size=0.2, random_state=1317)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68748\ttest-logloss:0.688019\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.640869\ttest-logloss:0.646349\n",
      "[20]\ttrain-logloss:0.606676\ttest-logloss:0.616444\n",
      "[30]\ttrain-logloss:0.580836\ttest-logloss:0.594589\n",
      "[40]\ttrain-logloss:0.561546\ttest-logloss:0.5785\n",
      "[50]\ttrain-logloss:0.546602\ttest-logloss:0.566265\n",
      "[60]\ttrain-logloss:0.534777\ttest-logloss:0.556676\n",
      "[70]\ttrain-logloss:0.52541\ttest-logloss:0.549253\n",
      "[80]\ttrain-logloss:0.517643\ttest-logloss:0.543283\n",
      "[90]\ttrain-logloss:0.511127\ttest-logloss:0.538703\n",
      "[100]\ttrain-logloss:0.505886\ttest-logloss:0.534731\n",
      "[110]\ttrain-logloss:0.501483\ttest-logloss:0.531589\n",
      "[120]\ttrain-logloss:0.497608\ttest-logloss:0.529153\n",
      "[130]\ttrain-logloss:0.494044\ttest-logloss:0.527198\n",
      "[140]\ttrain-logloss:0.490723\ttest-logloss:0.525586\n",
      "[150]\ttrain-logloss:0.487635\ttest-logloss:0.524478\n",
      "[160]\ttrain-logloss:0.484772\ttest-logloss:0.523315\n",
      "[170]\ttrain-logloss:0.482339\ttest-logloss:0.522523\n",
      "[180]\ttrain-logloss:0.479891\ttest-logloss:0.521763\n",
      "[190]\ttrain-logloss:0.477631\ttest-logloss:0.520812\n",
      "[200]\ttrain-logloss:0.475651\ttest-logloss:0.520016\n",
      "[210]\ttrain-logloss:0.473898\ttest-logloss:0.519539\n",
      "[220]\ttrain-logloss:0.472246\ttest-logloss:0.519261\n",
      "[230]\ttrain-logloss:0.470726\ttest-logloss:0.518957\n",
      "[240]\ttrain-logloss:0.469217\ttest-logloss:0.518564\n",
      "[250]\ttrain-logloss:0.467921\ttest-logloss:0.51817\n",
      "[260]\ttrain-logloss:0.466608\ttest-logloss:0.518019\n",
      "[270]\ttrain-logloss:0.465249\ttest-logloss:0.517658\n",
      "[280]\ttrain-logloss:0.463906\ttest-logloss:0.517535\n",
      "[290]\ttrain-logloss:0.462718\ttest-logloss:0.51731\n",
      "[300]\ttrain-logloss:0.461513\ttest-logloss:0.516956\n",
      "[310]\ttrain-logloss:0.46022\ttest-logloss:0.51673\n",
      "[320]\ttrain-logloss:0.459119\ttest-logloss:0.516354\n",
      "[330]\ttrain-logloss:0.458024\ttest-logloss:0.51598\n",
      "[340]\ttrain-logloss:0.456924\ttest-logloss:0.515739\n",
      "[350]\ttrain-logloss:0.455915\ttest-logloss:0.515356\n",
      "[360]\ttrain-logloss:0.454893\ttest-logloss:0.515047\n",
      "[370]\ttrain-logloss:0.453836\ttest-logloss:0.514734\n",
      "[380]\ttrain-logloss:0.452867\ttest-logloss:0.514548\n",
      "[390]\ttrain-logloss:0.451793\ttest-logloss:0.514265\n",
      "[399]\ttrain-logloss:0.450812\ttest-logloss:0.514057\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Running XGBoost\n",
    "##########################################\n",
    "# Set parameters for XGBoost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'test')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgboost_model.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saves Classifier\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(bst, 'xgboost_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loads Classifier\n",
    "from sklearn.externals import joblib\n",
    "bst = joblib.load('xgboost_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Test Dataset\n",
    "##########################################\n",
    "#Test Dataset\n",
    "data_test = pd.read_csv('/Users/Priscilla/Desktop/QuoraDataset/test.csv')\n",
    "#data_test = pd.read_csv('/stfm/research5/m1pll00/quora/dataset/test.csv')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Create the test features using FeatureUnion\n",
    "##########################################\n",
    "test_features = comb_features.transform([data_test['question1'][0:5000], data_test['question2'][0:5000]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "test = xgb.DMatrix(test_features)\n",
    "test_prediction = bst.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = data_test['test_id']\n",
    "sub['is_duplicate'] = test_prediction\n",
    "sub.to_csv('simple_xgb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
