{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Load Required Python Libraries\n",
    "##########################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pylev import levenshtein\n",
    "import re\n",
    "import nltk\n",
    "import chardet\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Dataset\n",
    "##########################################\n",
    "#Training Dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "data['question1'] = data['question1'].astype(str)\n",
    "data['question2'] = data['question2'].astype(str)\n",
    "y = data['is_duplicate']\n",
    "#Drop irrelevant features\n",
    "#data = data.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "df_train = data\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# q1_list = data['question1'][0:5000]\n",
    "# q2_list = data['question2'][0:5000]\n",
    "# total_questions = q1_list + q2_list\n",
    "# total_questions = [x for x in total_questions if type(x) != float]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "make_bow = vectorizer.fit_transform(total_questions)\n",
    "# LDA_clf = LDA(n_topics=200, learning_offset = 1.5)\n",
    "LDA_clf.fit(make_bow)\n",
    "q1_vec = vectorizer.transform(q1_list)\n",
    "q2_vec = vectorizer.transform(q2_list)\n",
    "lda_diff = LDA_clf.transform(q1_vec) - LDA_clf.transform(q2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 6, 5], [53, 46, 14, 12]],\n",
       "       [[-5, 4, 9], [44, 76, 8, 13]],\n",
       "       [[1, 6, 5], [60, 50, 14, 10]],\n",
       "       ..., \n",
       "       [[4, 7, 3], [67, 37, 14, 5]],\n",
       "       [[-1, 4, 5], [31, 34, 6, 6]],\n",
       "       [[3, 5, 2], [35, 25, 8, 6]]], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(np.array(total_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = ('lda', Pipeline([\n",
    "      ('counts', CountVectorizer()),\n",
    "      ('tf_idf', LDA(n_topics=200, learning_offset = 1.5))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsonwang/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('counts', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        ...        random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0))])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.fit(data['question1'][0:5000]+data['question2'][0:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Word count functions\n",
    "def shared_words(q1,q2):\n",
    "    question1_words = []\n",
    "    question2_words = []\n",
    "\n",
    "    for word in set(str(q1).lower().split()):\n",
    "        if word not in stop_words:\n",
    "            question1_words.append(word)\n",
    "\n",
    "    for word in set(str(q2).lower().split()):\n",
    "        if word not in stop_words:\n",
    "            question2_words.append(word)\n",
    "\n",
    "    #Question contains only stop words (or is an empty string)\n",
    "    if len(question1_words) == 0 or len(question2_words) == 0:\n",
    "        return 0\n",
    "\n",
    "    question1_shared_words = [w for w in question1_words if w in question2_words]\n",
    "    question2_shared_words = [w for w in question2_words if w in question1_words]\n",
    "\n",
    "    avg_words_shared = (len(question1_shared_words) + len(question2_shared_words))/(len(question1_words) + len(question2_words))\n",
    "    return avg_words_shared\n",
    "\n",
    "def word_length_diffs(q1,q2):\n",
    "    question1_words = []\n",
    "    question2_words = []\n",
    "\n",
    "    for word in set(str(q1).lower().split(' ')):\n",
    "        if word not in stop_words:\n",
    "            question1_words.append(word)\n",
    "\n",
    "    for word in set(str(q2).lower().split(' ')):\n",
    "        if word not in stop_words:\n",
    "            question2_words.append(word)\n",
    "\n",
    "    #Question contains only stop words (or is an empty string)\n",
    "\n",
    "    question_len_diff = (len(question1_words) - len(question2_words))\n",
    "    return [question_len_diff, len(question1_words), len(question2_words)]\n",
    "\n",
    "def word_lengths(q1,q2):\n",
    "    q1_char = len(str(q1).replace(' ', ''))\n",
    "    q2_char = len(str(q2).replace(' ', ''))\n",
    "\n",
    "    q1_words = len(str(q1).split())\n",
    "    q2_words = len(str(q2).split())\n",
    "    \n",
    "    return [q1_char, q2_char, q1_words, q2_words]\n",
    "def magic_features(df_1, df_2):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#These are the functions for our transformers\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "class LevDistanceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        \n",
    "        lev_distance_strings = [[a,b] \n",
    "        for a,b in zip(q1_list, q2_list)]\n",
    "        \n",
    "        lev_dist_array = np.array([\n",
    "    (float(levenshtein(pair[0], pair[1]))/\n",
    "    (float(sum([x.count('') for x in pair[0]])) + \n",
    "    float(sum([x.count('') for x in pair[1]])))) \n",
    "    for pair in lev_distance_strings \n",
    "        ])\n",
    "        \n",
    "        return lev_dist_array.reshape(len(lev_dist_array),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class TfIdfDiffTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_questions = q1_list + q2_list\n",
    "        total_questions = [x for x in total_questions if type(x) != float]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words = 'english', vocabulary = total_words)\n",
    "        vectorizer.fit(total_questions)\n",
    "\n",
    "        return vectorizer.transform(q1_list) - vectorizer.transform(q2_list)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class LDATransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_questions = q1_list + q2_list\n",
    "        total_questions = [x for x in total_questions if type(x) != float]\n",
    "        \n",
    "        vectorizer = CountVectorizer(stop_words='english')\n",
    "        make_bow = vectorizer.fit_transform(total_questions)\n",
    "        LDA_clf = LDA(n_topics=300, learning_offset = 1.5)\n",
    "        LDA_clf.fit(make_bow)\n",
    "        q1_vec = vectorizer.transform(q1_list)\n",
    "        q2_vec = vectorizer.transform(q2_list)\n",
    "        #lda_diff = LDA_clf.transform(q1_vec) - LDA_clf.transform(q2_vec)\n",
    "        \n",
    "        return q1_vec - q2_vec\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class CosineDistTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_questions = q1_list + q2_list\n",
    "        total_questions = [x for x in total_questions if type(x) != float]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "        vectorizer.fit(total_questions)\n",
    "        \n",
    "        q1_tf = vectorizer.transform(q1_list) \n",
    "        q2_tf = vectorizer.transform(q2_list)\n",
    "        cos_sim = []\n",
    "        for i in range(0,len(q1_list)):\n",
    "            cos_sim.append(cosine_similarity(q1_tf[i], q2_tf[i])[0][0])\n",
    "            \n",
    "        return np.array(cos_sim).reshape(len(cos_sim),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class AverageSharedWords(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        avg_words = [shared_words(q1,q2) for q1, q2 in zip(q1_list, q2_list)]\n",
    "            \n",
    "        return np.array(avg_words).reshape(len(avg_words),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class WordLength(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the length features for each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_features = []\n",
    "        \n",
    "        word_length_diff = [word_length_diffs(q1,q2) for q1, q2 in zip(q1_list, q2_list)]\n",
    "        #character count\n",
    "        len_features = [word_lengths(q1,q2) for q1, q2 in zip(q1_list, q2_list)]\n",
    "        zipped_features = [[x,y] for x,y in zip(word_length_diff,len_features)]\n",
    "        \n",
    "        for i in range(len(zipped_features)):\n",
    "            total_features.append([j for i in zipped_features[i] for j in i])\n",
    "        return np.array(total_features)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "# vectorizer.fit(data['question1'] + data['question2'])\n",
    "# total_words = list(set(vectorizer.get_feature_names()))\n",
    "\n",
    "comb_features = FeatureUnion([\n",
    "                              ('lda_diff',LDATransformer()),\n",
    "                              ('cos_diff',CosineDistTransformer()), \n",
    "                              ('lev_diff', LevDistanceTransformer()),\n",
    "                              ('avg_words', AverageSharedWords()),\n",
    "                              ('word_len', WordLength()),\n",
    "                              ('tf_diff', TfIdfDiffTransformer())\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsonwang/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Split the dataset into training and testing datasets\n",
    "##########################################\n",
    "y = data.ix[:,'is_duplicate']\n",
    "all_features = comb_features.fit_transform([data['question1'], data['question2']])\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, y, test_size=0.2, random_state=21460)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Running XGBoost\n",
    "##########################################\n",
    "# Set parameters for XGBoost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 8\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'test')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Features importances...\")\n",
    "importance = bst.get_fscore(fmap='xgb.fmap')\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "ft = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "\n",
    "ft.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 25))\n",
    "plt.gcf().savefig('features_importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Saves Classifier\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(bst, 'xgboost_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loads Classifier\n",
    "from sklearn.externals import joblib\n",
    "bst = joblib.load('xgboost_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Test Dataset\n",
    "##########################################\n",
    "#Test Dataset\n",
    "data_test = pd.read_csv('/Users/Priscilla/Desktop/QuoraDataset/test.csv')\n",
    "#data_test = pd.read_csv('/stfm/research5/m1pll00/quora/dataset/test.csv')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Create the test features using FeatureUnion\n",
    "##########################################\n",
    "test_features = comb_features.transform([data_test['question1'][0:5000], data_test['question2'][0:5000]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "test = xgb.DMatrix(test_features)\n",
    "test_prediction = bst.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = data_test['test_id']\n",
    "sub['is_duplicate'] = test_prediction\n",
    "sub.to_csv('simple_xgb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
