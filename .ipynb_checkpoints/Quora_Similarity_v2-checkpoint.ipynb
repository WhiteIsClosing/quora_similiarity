{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Load Required Python Libraries\n",
    "##########################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import chardet\n",
    "import itertools\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Dataset\n",
    "##########################################\n",
    "#Training Dataset\n",
    "data = pd.read_csv('/stfm/research5/m1pll00/quora/dataset/train.csv')\n",
    "#data = pd.read_csv('/Users/Priscilla/Desktop/QuoraDataset/train.csv')\n",
    "\n",
    "#Drop irrelevant features\n",
    "data = data.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Feature Engineering Method 1: \n",
    "# Creating new input variables to improve ML algorithm performance\n",
    "##########################################\n",
    "#Feature: Length of Question\n",
    "#Apply length function to every data.question1 & data.question2 observation\n",
    "data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
    "data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
    "\n",
    "#Feature: Difference in length between the Questions\n",
    "#Substract len_q1 from len_q2\n",
    "data['len_diff'] = data.len_q1 - data.len_q2\n",
    "\n",
    "#Feature: Character count of Question\n",
    "#Strip whitespace in data.question1 & data.question2 and apply the length function\n",
    "data['len_char_q1'] = data.question1.apply(lambda x: len(str(x).replace(' ', '')))\n",
    "data['len_char_q2'] = data.question2.apply(lambda x: len(str(x).replace(' ', '')))\n",
    "\n",
    "#Feature: Word count of Question\n",
    "#Call split function on every data.question1 & data.question2 observation and apply the length function\n",
    "data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#Feature: Common words between the Questions\n",
    "#Intersection of data.question1 and data.question2\n",
    "#Set function is applied so repeated words in a question is omitted from the final common word count\n",
    "#Axis=1 to calculate the means column-wise (-->) rather than the default of Axis=0 to calculate the means row-wise(v)\n",
    "data['len_common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Feature Engineering Method 2:\n",
    "# Create Bag Of Words Model with Tfidf Normalization\n",
    "##########################################\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Obtain the complete vocabulary for the entire dataset\n",
    "questions_combined = list(data.ix[:,'question1'].values.astype('str')) + list(data.ix[:,'question2'].values.astype('str'))\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(questions_combined)\n",
    "\n",
    "#86153 unique vocabulary words question1 & question2 combined\n",
    "complete_vocab = vectorizer.get_feature_names()\n",
    "\n",
    "#Generate tfidf values for question1 and question2 based on the complete vocabulary of the dataset\n",
    "vectorizer_q1 = TfidfVectorizer(vocabulary = complete_vocab)\n",
    "vectorizer_q2 = TfidfVectorizer(vocabulary = complete_vocab)\n",
    "tfidf_question1 = vectorizer_q1.fit_transform(data.question1.values.astype('str'))\n",
    "tfidf_question2 = vectorizer_q2.fit_transform(data.question2.values.astype('str'))\n",
    "\n",
    "#Substract the difference of the tfidf weight matricies for the two questions\n",
    "#Will be 0 if the words are weighted the same in both questions (similar significance contribution)\n",
    "diff_idf = tfidf_question1 - tfidf_question2\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Feature Engineering Method 3:\n",
    "# Word2Vec Model\n",
    "##########################################\n",
    "#Training own word2vec model based on the training data we have\n",
    "#Do not remove numbers or stop words so the algorithm can have a broader context of the sentence to produce higher quality vectors\n",
    "\n",
    "#Tokenizer for sentence splitting\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#Function to prep question1 and question2 for word2vec model\n",
    "#Word2vec expects a list of lists as input (single sentences each as a list of words)\n",
    "def question_to_wordlist(text, remove_stopwords = False):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "def question_to_sentences(text, tokenizer, remove_stopwords = False):\n",
    "    text = tokenizer.tokenize(text.strip())\n",
    "    sentences = []\n",
    "    \n",
    "    for t in text:\n",
    "        if(len(t) > 0):\n",
    "            sentences.append(question_to_wordlist(t, remove_stopwords))\n",
    "    return sentences\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Encoding Detection library\n",
    "import chardet\n",
    "\n",
    "#Prep data for word2vec\n",
    "sentences = []\n",
    "\n",
    "print(\"Parsing sentences from training set...\")\n",
    "#Converting question1 to sentences for word2vec model\n",
    "for i in range(0, len(data['question1'])):\n",
    "    try:\n",
    "        #Check for empty strings \"\"\n",
    "        if(not pd.isnull(data['question1'][i])):\n",
    "            sentences += question_to_sentences(data['question1'][i], tokenizer)\n",
    "    except:\n",
    "        try:\n",
    "            encoding = chardet.detect(data['question1'][i])['encoding']\n",
    "            sentences += question_to_sentences(data['question1'][i].decode(encoding), tokenizer)\n",
    "        except:\n",
    "            print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting question2 to sentences for word2vec model\n",
    "for i in range(0,len(data['question2'])):\n",
    "    try:\n",
    "        if(not pd.isnull(data['question2'][i])):\n",
    "            sentences += question_to_sentences(data['question2'][i], tokenizer)\n",
    "    except:\n",
    "        try:\n",
    "            encoding = chardet.detect(data['question2'][i])['encoding']\n",
    "            sentences += question_to_sentences(data['question2'][i].decode(encoding), tokenizer)\n",
    "        except:\n",
    "            print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Library for printing output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#Set parameters for word2vec model\n",
    "num_features = 300 #Word vector dimensionality\n",
    "min_word_count = 10 #Minimum word count (min times a word has to appear to be meaningful, should be between 0-100 depending on dataset size)\n",
    "num_workers = 4 #Number of threads to run in parallel (only useful if have cython installed)\n",
    "context = 5 #Context window size (how many words apart from current one can affect the meaning of the current word)\n",
    "downsampling = 1e-3 #Downsample setting for frequent words (words that appear with higher frequency will be randomly down-sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Library for training word2vec model\n",
    "from gensim.models import word2vec\n",
    "\n",
    "#Training word2vec model\n",
    "print(\"Training word2vec model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers = num_workers,\\\n",
    "                         size = num_features, min_count = min_word_count,\\\n",
    "                         window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If not training the model any further call init_sims to make the model memory-efficient\n",
    "model.init_sims(replace = True)\n",
    "\n",
    "#Save the model\n",
    "model_name = \"300features_10minwords_5context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20224, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(# words in model's vocab, size of feature vector)\n",
    "#Library for training word2vec model\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load(\"300features_10minwords_5context\")\n",
    "model.wv.syn0.shape\n",
    "# model[\"flower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis\n",
    "#Function to find the following distributional summary statistics:\n",
    "# 1 - Mean\n",
    "# 2 - Median\n",
    "def makeDistributionalFeatures(data, model):\n",
    "    features = []\n",
    "    remove_index = []\n",
    "    \n",
    "    #For each question in the dataset:\n",
    "    # 1 - Compute similarity metric from word2vec model using every word combination between question1 and question2\n",
    "    # 2 - Create the distributional summary statistics for every combination of the disimilar words\n",
    "    for index in tqdm(xrange(0, len(data))):\n",
    "            #Convert question1 and question2 into a list of words\n",
    "            question1 = question_to_wordlist(str(data.question1[index]))\n",
    "            question2 = question_to_wordlist(str(data.question2[index]))\n",
    "            \n",
    "            #Finds every word combination between question1 and question2\n",
    "            combinations = list(itertools.product(question1, question2))\n",
    "            combinations = [list(combination) for combination in combinations]\n",
    "            \n",
    "            #Tracks word2vec similarity metric for every word combination\n",
    "            values = []\n",
    "            \n",
    "            #Loops through each word combination\n",
    "            for combination in combinations:\n",
    "                #Checks if the model contains the words in its vocabulary\n",
    "                # 1 - Yes, adds it to the values list to calculate distributional stats with\n",
    "                # 2 - No, go to the next word pair\n",
    "                try:\n",
    "                    values.append(model.wv.similarity(combination[0], combination[1]))\n",
    "                except KeyError:\n",
    "                    pass\n",
    "      \n",
    "            #If there is at least one similarity metric calculate its mean and median\n",
    "            if(len(values) >= 1):\n",
    "                features.append([np.mean(values), np.median(values), np.std(values), kurtosis(values)])\n",
    "            else:\n",
    "                #Append the current index to remove_index to track indicies we will not fit the model to\n",
    "                # 1 - The only combination contained a word our model does not contain\n",
    "                # 2 - Question1 or Question2 or both were \"\"\n",
    "                # 3 - We could not tokenize either Question1 or Question2\n",
    "                remove_index.append(index)\n",
    "            \n",
    "    return(features, remove_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404290/404290 [26:56<00:00, 250.06it/s]\n"
     ]
    }
   ],
   "source": [
    "word2vec_feature, remove = makeDistributionalFeatures(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_feature_df = pd.DataFrame(word2vec_feature, columns=[\"mean\", \"median\", \"standardDev\", \"kurtosis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove remove index observations from diff_idf prior to merging:\n",
    "#Recreate diff_idf without remove index\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "question1_df = data.ix[:,'question1']\n",
    "question1_df = question1_df.drop(question1_df.index[remove])\n",
    "\n",
    "question2_df = data.ix[:,'question2']\n",
    "question2_df = question2_df.drop(question2_df.index[remove])\n",
    "\n",
    "#Obtain the complete vocabulary for the entire dataset\n",
    "questions_combined = list(question1_df.values.astype('str')) + list(question2_df.values.astype('str'))\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit(questions_combined)\n",
    "\n",
    "#86153 unique vocabulary words question1 & question2 combined\n",
    "complete_vocab = vectorizer.get_feature_names()\n",
    "\n",
    "#Generate tfidf values for question1 and question2 based on the complete vocabulary of the dataset\n",
    "tfidf_question1 = vectorizer.transform(question1_df.values.astype('str'))\n",
    "tfidf_question2 = vectorizer.transform(question2_df.values.astype('str'))\n",
    "\n",
    "#Substract the difference of the tfidf weight matricies for the two questions\n",
    "#Will be 0 if the words are weighted the same in both questions (similar significance contribution)\n",
    "diff_idf = tfidf_question1 - tfidf_question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove remove index observations from data created features prior to merging:\n",
    "data_features = data.ix[:,'len_q1':'len_common_words']\n",
    "data_features = data_features.drop(data_features.index[remove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge all the features together\n",
    "import scipy\n",
    "merged_features = scipy.sparse.hstack(blocks=[diff_idf, scipy.sparse.csr_matrix(data_features), scipy.sparse.csr_matrix(word2vec_feature_df)])\n",
    "\n",
    "#Remove dropped observations from data\n",
    "data_df = data.ix[:,'question1':'question2']\n",
    "data_df = data_df.drop(data_df.index[remove])\n",
    "outcome_df = data.ix[:,'is_duplicate']\n",
    "outcome_df = outcome_df.drop(outcome_df.index[remove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Split the dataset into training and testing datasets\n",
    "##########################################\n",
    "#Loads the library required for splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Method 1 Features\n",
    "# features = data.ix[:,'len_q1':]\n",
    "\n",
    "#Method 2 Features\n",
    "# features = diff_idf\n",
    "# y = data.ix[:,'is_duplicate']\n",
    "\n",
    "#Method 3 Features\n",
    "features = merged_features\n",
    "y = outcome_df\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged_features, y, test_size=0.2)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Classifier\n",
    "##########################################\n",
    "#Loads required libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Create a SVC (clf by convention = 'classifier')\n",
    "clf = SVC(kernel=\"linear\")\n",
    "\n",
    "#Train the Classifier\n",
    "clf.fit(X_train, y_train)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saves Classifier\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Apply Classifer on the testing split of the dataset\n",
    "##########################################\n",
    "#Predicts the outcome variable of the testing split of the dataset\n",
    "test_prediction = clf.predict(X_test)\n",
    "#print(test_prediction)\n",
    "\n",
    "#Prediction probability for the value of the outcome variable (0 or 1)\n",
    "test_prediction_proba = clf.predict_proba(X_test)\n",
    "#print(test_prediction_proba)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.781546072975\n"
     ]
    }
   ],
   "source": [
    "#Displays the accuracy score of the Classifier on the test split\n",
    "clf_accuracy = accuracy_score(y_test, test_prediction)\n",
    "print(clf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60101514084837504"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################\n",
    "# Apply Logloss function to Test Dataset Output\n",
    "##########################################\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss(np.array(y_test), test_prediction_proba)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Convert question1 and question2 into a list of words\n",
    "# question1 = question_to_wordlist(data.question1[105781])\n",
    "# question2 = question_to_wordlist(data.question2[105781])\n",
    "# print(question1)\n",
    "# print(question2)\n",
    "# combinations = list(itertools.product(question1, question2))\n",
    "# combinations = [list(combination) for combination in combinations]\n",
    "# combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Function to find the following distributional summary statistics:\n",
    "# # 1 - Mean\n",
    "# # 2 - Median\n",
    "# def makeDistributionalFeatures(data, model):\n",
    "#     features = []\n",
    "#     notDup_index = []\n",
    "#     isDup_index = []\n",
    "#     remove_index = []\n",
    "    \n",
    "#     #For each question in the dataset:\n",
    "#     # 1 - Find the disimilar words\n",
    "#     # 2 - Create the distributional summary statistics for every combination of the disimilar words\n",
    "#     for index in tqdm(xrange(0, len(data))):\n",
    "#         disimilarWords = findDisimilarWords(data.question1[index], data.question2[index])\n",
    "        \n",
    "#         #Append the current index to notDup_index to track indices we will manually set is_duplicate result to 0\n",
    "#         # 1 - When we cannot decode either question1 or question2\n",
    "#         # 2 - When we have a \"\" as question1 or question2\n",
    "#         if(disimilarWords == 0):\n",
    "#             notDup_index.append(index)\n",
    "#             features.append([-1, -1])\n",
    "#         #Append the current index to isDup_index to track indicies we will manually set is_duplicate result to 1\n",
    "#         # 1 - When question1 and question2 are the same question\n",
    "#         elif(len(disimilarWords) == 0):\n",
    "#             isDup_index.append(index)\n",
    "#             features.append([1, 1])\n",
    "#         #Calculate the distributional summary statistics for every combination of the disimilar words\n",
    "#         else:\n",
    "#             combinations = list(itertools.combinations(disimilarWords, 2))\n",
    "#             combinations = [list(combination) for combination in combinations]\n",
    "#             values = []\n",
    "            \n",
    "#             disimilarWords = list(disimilarWords)\n",
    "\n",
    "#             for word in disimilarWords:\n",
    "#                 word_val = []\n",
    "#                 for combination in combinations:\n",
    "#                     if(combination[0] == word):\n",
    "#                         #Checks if the model contains the words in its vocabulary\n",
    "#                         try:\n",
    "#                             word_val.append(model.wv.similarity(combination[0], combination[1]))\n",
    "#                         except KeyError:\n",
    "#                             pass\n",
    "#                 #Keep only the largest similarity metric for each word\n",
    "#                 if(len(word_val) > 0):\n",
    "#                     max_sim = max(word_val)\n",
    "#                     values.append(max_sim)\n",
    "      \n",
    "#             #If there is at least one similarity metric calculate its mean and median\n",
    "#             if(len(values) >= 1):\n",
    "#                 features.append([np.mean(values), np.median(values)])\n",
    "#             else:\n",
    "#                 #Append the current index to remove_index to track indicies we will not fit the model to\n",
    "#                 # 1 - The only combination contained a word our model does not contain\n",
    "#                 remove_index.append(index)\n",
    "#                 features.append([-1, -1])\n",
    "            \n",
    "#     return(features, notDup_index, isDup_index, remove_index)\n",
    "\n",
    "# #Function to find the disimilar words between question1 and question2\n",
    "# def findDisimilarWords(question1, question2):\n",
    "#     disimilarWords = []\n",
    "    \n",
    "#     #If either question1 or question2 is an empty string \"\" return an empty list\n",
    "#     if(pd.isnull(question1) | pd.isnull(question2)):\n",
    "#         return(0)\n",
    "    \n",
    "#     #Otherwise find the list of disimilar words between question1 and question2\n",
    "#     else:\n",
    "#         #Decodes question1 and question2\n",
    "#         try:\n",
    "#             encoding = chardet.detect(question1)['encoding']\n",
    "#             question1 = question1.decode(encoding)\n",
    "            \n",
    "#             encoding = chardet.detect(question2)['encoding']\n",
    "#             question2 = question2.decode(encoding)\n",
    "#         #Unable to decode question1 or question2\n",
    "#         #Return an empty list\n",
    "#         except:\n",
    "#             return(0)\n",
    "        \n",
    "#         #Convert question1 and question2 into a list of words\n",
    "#         question1 = question_to_wordlist(question1)\n",
    "#         question2 = question_to_wordlist(question2)\n",
    "\n",
    "#         #Find the dismilar words\n",
    "#         disimilarWords = set(\" \".join(question1).split()).symmetric_difference(set(\" \".join(question2).split()))\n",
    "#         return(disimilarWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2vec_feature, notDup, isDup, remove = makeDistributionalFeatures(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import statistics\n",
    "# words = findDisimilarWords(data['question1'][105781], data['question2'][105781])\n",
    "# combinations = list(itertools.combinations(words, 2))\n",
    "# combinations = [list(item) for item in combinations]\n",
    "# values = []\n",
    "# features = []\n",
    "# print(combinations)\n",
    "\n",
    "# word_list = list(words)\n",
    "# print(word_list)\n",
    "\n",
    "# for word in word_list:\n",
    "#     word_val = []\n",
    "#     for combination in combinations:\n",
    "# #         print(\"Combination: \" + combination[0])\n",
    "# #         print(\"Word: \" + word)\n",
    "#         if(combination[0] == word):\n",
    "# #             print(\"Add!\")\n",
    "#             word_val.append(model.wv.similarity(combination[0], combination[1]))\n",
    "#     if(len(word_val) > 0):\n",
    "#         max_sim = max(word_val)\n",
    "#         values.append(max_sim)\n",
    "    \n",
    "# print(values)\n",
    "\n",
    "#print(unique(combinations))\n",
    "# for i in xrange(0, len(combinations)):\n",
    "# #         print(combinations[i])\n",
    "#         values.append(model.wv.similarity(combinations[i][0], combinations[i][1]))\n",
    "# print(values)\n",
    "# print(statistics.stdev(values))\n",
    "# print(statistics.mean(values))\n",
    "# print(statistics.median(values))\n",
    "# features.append([statistics.stdev(values), statistics.mean(values), statistics.median(values)])\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Average word vectors for each question\n",
    "# def questionFeatureVec(question, model, num_features):\n",
    "#     featureVec = np.zeros((num_features,), dtype = \"float32\")\n",
    "#     num_words = 0\n",
    "    \n",
    "#     #Creates a set with the list of words in the model's vocab.\n",
    "#     vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "#     #Loop through each word in the question and if it's in the model's vocab. add the word's feature vector to the total\n",
    "#     for word in question:\n",
    "#         if word in vocabulary:\n",
    "#             num_words = num_words + 1\n",
    "#             featureVec = np.add(featureVec, model[word])\n",
    "    \n",
    "#     try:\n",
    "#         #Divide the result by the total number of words in the question to get the average\n",
    "#         featureVec = np.divide(featureVec, num_words)\n",
    "#     except RuntimeWarning:\n",
    "#         print(featureVec)\n",
    "#     return(featureVec)\n",
    "\n",
    "# #Create average word vector for entire dataset\n",
    "# def makeFeatureVec(questions, model, num_features):\n",
    "#     count = 0\n",
    "#     dataFeatureVec = np.zeros((len(questions), num_features), dtype = \"float32\")\n",
    "    \n",
    "#     #Loop through each question in the dataset and calculate its average questions word vectors\n",
    "#     for question in questions:\n",
    "#         dataFeatureVec[count] = questionFeatureVec(question, model, num_features)\n",
    "#         count += 1\n",
    "#     return(dataFeatureVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the average word vectors for question1\n",
    "# clean_question1 = []\n",
    "# for question in data.question1:\n",
    "#     clean_question1.append(question_to_wordlist(question, remove_stopwords = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vecs_question1 = makeFeatureVec(clean_question1, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the average word vectors for question2\n",
    "# clean_question2 = []\n",
    "# for question in data.question2:\n",
    "#     clean_question2.append(question_to_wordlist(question, remove_stopwords = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vecs_question2 = makeFeatureVec(clean_question2, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vecs_merge = [vecs_question1, vecs_question2]\n",
    "# len(vecs_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(data['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit a Random Forest Classifier to the dataset - Using 100 trees\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# forest = RandomForestClassifier(n_estimators = 100)\n",
    "# forest = forest.fit(vecs_merge, data['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.similarity('woman', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import chardet\n",
    "# result = chardet.detect(data['question1'][11077])['encoding']\n",
    "# print(result)\n",
    "# print(data['question1'][11077].decode('Windows-1254'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preview newly added features to the dataset\n",
    "# pd.options.display.max_colwidth = 100\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Features of the training dataset\n",
    "# data.ix[:,'len_q1':].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data.ix[:,'len_q1':].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Outcome variable of the training dataset\n",
    "# data.ix[:,'is_duplicate'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data.ix[:,'is_duplicate'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preview Prediction\n",
    "# test_prediction[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preview the Prediction Probability [0, 1]\n",
    "# test_prediction_proba[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Displays Results in a Confusion Matrix\n",
    "#Anything on the diagonal was classified correctly and anything off the diagonal was classified incorrectly\n",
    "# pd.crosstab(y_test, test_prediction, rownames=['Actual Similarity'], colnames=['Predicted Similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Displays a list of features that were the most important in affecting the accuracy of the classification\n",
    "# important_features = list(zip(X_train, clf.feature_importances_))\n",
    "# important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Test Dataset\n",
    "##########################################\n",
    "#Test Dataset\n",
    "#data_test = pd.read_csv('/Users/Priscilla/Desktop/QuoraDataset/test.csv')\n",
    "data_test = pd.read_csv('/stfm/research5/m1pll00/quora/dataset/test.csv')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Feature Engineering Method 1: \n",
    "# Creating new input variables to improve ML algorithm performance\n",
    "##########################################\n",
    "#Feature: Length of Question\n",
    "#Apply length function to every data.question1 & data.question2 observation\n",
    "data_test['len_q1'] = data_test.question1.apply(lambda x: len(str(x)))\n",
    "data_test['len_q2'] = data_test.question2.apply(lambda x: len(str(x)))\n",
    "\n",
    "#Feature: Difference in length between the Questions\n",
    "#Substract len_q1 from len_q2\n",
    "data_test['len_diff'] = data_test.len_q1 - data_test.len_q2\n",
    "\n",
    "#Feature: Character count of Question\n",
    "#Strip whitespace in data.question1 & data.question2 and apply the length function\n",
    "data_test['len_char_q1'] = data_test.question1.apply(lambda x: len(str(x).replace(' ', '')))\n",
    "data_test['len_char_q2'] = data_test.question2.apply(lambda x: len(str(x).replace(' ', '')))\n",
    "\n",
    "#Feature: Word count of Question\n",
    "#Call split function on every data.question1 & data.question2 observation and apply the length function\n",
    "data_test['len_word_q1'] = data_test.question1.apply(lambda x: len(str(x).split()))\n",
    "data_test['len_word_q2'] = data_test.question2.apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#Feature: Common words between the Questions\n",
    "#Intersection of data.question1 and data.question2\n",
    "#Set function is applied so repeated words in a question is omitted from the final common word count\n",
    "#Axis=1 to calculate the means column-wise (-->) rather than the default of Axis=0 to calculate the means row-wise(v)\n",
    "data_test['len_common_words'] = data_test.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_feature_test, remove_test = makeDistributionalFeatures(data_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_feature_df = pd.DataFrame(word2vec_feature_test, columns=[\"mean\", \"median\", \"standardDev\", \"kurtosis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove remove index observations from diff_idf prior to merging:\n",
    "#Recreate diff_idf without remove index\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "question1_df = data_test.ix[:,'question1']\n",
    "question1_df = question1_df.drop(question1_df.index[remove_test])\n",
    "\n",
    "question2_df = data_test.ix[:,'question2']\n",
    "question2_df = question2_df.drop(question2_df.index[remove_test])\n",
    "\n",
    "#Obtain the complete vocabulary for the entire dataset\n",
    "questions_combined = list(question1_df.values.astype('str')) + list(question2_df.values.astype('str'))\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit(questions_combined)\n",
    "\n",
    "#86153 unique vocabulary words question1 & question2 combined\n",
    "complete_vocab = vectorizer.get_feature_names()\n",
    "\n",
    "#Generate tfidf values for question1 and question2 based on the complete vocabulary of the dataset\n",
    "tfidf_question1 = vectorizer.transform(question1_df.values.astype('str'))\n",
    "tfidf_question2 = vectorizer.transform(question2_df.values.astype('str'))\n",
    "\n",
    "#Substract the difference of the tfidf weight matricies for the two questions\n",
    "#Will be 0 if the words are weighted the same in both questions (similar significance contribution)\n",
    "diff_idf = tfidf_question1 - tfidf_question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove remove index observations from data created features prior to merging:\n",
    "data_features = data_test.ix[:,'len_q1':'len_common_words']\n",
    "data_features = data_features.drop(data_features.index[remove_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge all the features together\n",
    "import scipy\n",
    "merged_features = scipy.sparse.hstack(blocks=[diff_idf, scipy.sparse.csr_matrix(data_features), scipy.sparse.csr_matrix(word2vec_feature_df)])\n",
    "\n",
    "#Remove dropped observations from data\n",
    "data_df = data_test.ix[:,'question1':'question2']\n",
    "data_df = data_df.drop(data_df.index[remove_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Apply Classifer on the testing split of the dataset\n",
    "##########################################\n",
    "#Predicts the outcome variable of the testing split of the dataset\n",
    "test_prediction = clf.predict(data_df)\n",
    "#print(test_prediction)\n",
    "\n",
    "#Prediction probability for the value of the outcome variable (0 or 1)\n",
    "test_prediction_proba = clf.predict_proba(data_df)\n",
    "#print(test_prediction_proba)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Create Submission File\n",
    "##########################################\n",
    "submission = pd.DataFrame()\n",
    "submission['test_id'] = data_test.test_id\n",
    "submission['is_duplicate'] = test_prediction_proba\n",
    "\n",
    "submission.to_csv('/Users/Priscilla/Desktop/QuoraDataset/submission.csv', index = False)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
