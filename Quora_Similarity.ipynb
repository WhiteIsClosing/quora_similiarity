{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Load Required Python Libraries\n",
    "##########################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Dataset\n",
    "##########################################\n",
    "#Training Dataset\n",
    "data = pd.read_csv('/stfm/research5/m1pll00/quora/dataset/train.csv')\n",
    "\n",
    "#Drop irrelevant features\n",
    "data = data.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Feature Engineering Method 1: \n",
    "# Creating new input variables to improve ML algorithm performance\n",
    "##########################################\n",
    "#Feature: Length of Question\n",
    "#Apply length function to every data.question1 & data.question2 observation\n",
    "data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
    "data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
    "\n",
    "#Feature: Difference in length between the Questions\n",
    "#Substract len_q1 from len_q2\n",
    "data['len_diff'] = data.len_q1 - data.len_q2\n",
    "\n",
    "#Feature: Character count of Question\n",
    "#Strip whitespace in data.question1 & data.question2 and apply the length function\n",
    "data['len_char_q1'] = data.question1.apply(lambda x: len(str(x).replace(' ', '')))\n",
    "data['len_char_q2'] = data.question2.apply(lambda x: len(str(x).replace(' ', '')))\n",
    "\n",
    "#Feature: Word count of Question\n",
    "#Call split function on every data.question1 & data.question2 observation and apply the length function\n",
    "data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#Feature: Common words between the Questions\n",
    "#Intersection of data.question1 and data.question2\n",
    "#Set function is applied so repeated words in a question is omitted from the final common word count\n",
    "#Axis=1 to calculate the means column-wise (-->) rather than the default of Axis=0 to calculate the means row-wise(v)\n",
    "data['len_common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Feature Engineering Method 2:\n",
    "# Create Bag Of Words Model with Tfidf Normalization\n",
    "##########################################\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Obtain the complete vocabulary for the entire dataset\n",
    "questions_combined = list(data.ix[:,'question1'].values.astype('str')) + list(data.ix[:,'question2'].values.astype('str'))\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(questions_combined)\n",
    "\n",
    "#86153 unique vocabulary words question1 & question2 combined\n",
    "complete_vocab = vectorizer.get_feature_names()\n",
    "\n",
    "#Generate tfidf values for question1 and question2 based on the complete vocabulary of the dataset\n",
    "vectorizer_q1 = TfidfVectorizer(vocabulary = complete_vocab)\n",
    "vectorizer_q2 = TfidfVectorizer(vocabulary = complete_vocab)\n",
    "tfidf_question1 = vectorizer_q1.fit_transform(data.question1.values.astype('str'))\n",
    "tfidf_question2 = vectorizer_q2.fit_transform(data.question2.values.astype('str'))\n",
    "\n",
    "#Substract the difference of the tfidf weight matricies for the two questions\n",
    "#Will be 0 if the words are weighted the same in both questions (similar significance contribution)\n",
    "diff_idf = tfidf_question1 - tfidf_question2\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Feature Engineering Method 3:\n",
    "# Word2Vec Model\n",
    "##########################################\n",
    "#Training own word2vec model based on the training data we have\n",
    "#Do not remove numbers or stop words so the algorithm can have a broader context of the sentence to produce higher quality vectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "\n",
    "#Tokenizer for sentence splitting\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#Function to prep question1 and question2 for word2vec model\n",
    "#Word2vec expects a list of lists as input (single sentences each as a list of words)\n",
    "def question_to_wordlist(text, remove_stopwords = False):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "def question_to_sentences(text, tokenizer, remove_stopwords = False):\n",
    "    text = tokenizer.tokenize(text.strip())\n",
    "    sentences = []\n",
    "    \n",
    "    for t in text:\n",
    "        if(len(t) > 0):\n",
    "            sentences.append(question_to_wordlist(t, remove_stopwords))\n",
    "    return sentences\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Encoding Detection library\n",
    "import chardet\n",
    "\n",
    "#Prep data for word2vec\n",
    "sentences = []\n",
    "\n",
    "print(\"Parsing sentences from training set...\")\n",
    "#Converting question1 to sentences for word2vec model\n",
    "for i in xrange(0, len(data['question1'])):\n",
    "    try:\n",
    "        #Check for empty strings \"\"\n",
    "        if(not pd.isnull(data['question1'][i])):\n",
    "            sentences += question_to_sentences(data['question1'][i], tokenizer)\n",
    "    except:\n",
    "        try:\n",
    "            encoding = chardet.detect(data['question1'][i])['encoding']\n",
    "            sentences += question_to_sentences(data['question1'][i].decode(encoding), tokenizer)\n",
    "        except:\n",
    "            print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting question2 to sentences for word2vec model\n",
    "for i in xrange(0,len(data['question2'])):\n",
    "    try:\n",
    "        if(not pd.isnull(data['question2'][i])):\n",
    "            sentences += question_to_sentences(data['question2'][i], tokenizer)\n",
    "    except:\n",
    "        try:\n",
    "            encoding = chardet.detect(data['question2'][i])['encoding']\n",
    "            sentences += question_to_sentences(data['question2'][i].decode(encoding), tokenizer)\n",
    "        except:\n",
    "            print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Library for printing output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#Set parameters for word2vec model\n",
    "num_features = 300 #Word vector dimensionality\n",
    "min_word_count = 10 #Minimum word count (min times a word has to appear to be meaningful, should be between 0-100 depending on dataset size)\n",
    "num_workers = 4 #Number of threads to run in parallel (only useful if have cython installed)\n",
    "context = 5 #Context window size (how many words apart from current one can affect the meaning of the current word)\n",
    "downsampling = 1e-3 #Downsample setting for frequent words (words that appear with higher frequency will be randomly down-sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Library for training word2vec model\n",
    "from gensim.models import word2vec\n",
    "\n",
    "#Training word2vec model\n",
    "print(\"Training word2vec model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers = num_workers,\\\n",
    "                         size = num_features, min_count = min_word_count,\\\n",
    "                         window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If not training the model any further call init_sims to make the model memory-efficient\n",
    "model.init_sims(replace = True)\n",
    "\n",
    "#Save the model\n",
    "model_name = \"300features_10minwords_5context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20224, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(# words in model's vocab, size of feature vector)\n",
    "#Library for training word2vec model\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load(\"300features_10minwords_5context\")\n",
    "model.wv.syn0.shape\n",
    "# model[\"flower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Average word vectors for each question\n",
    "def questionFeatureVec(question, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype = \"float32\")\n",
    "    num_words = 0\n",
    "    \n",
    "    #Creates a set with the list of words in the model's vocab.\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    #Loop through each word in the question and if it's in the model's vocab. add the word's feature vector to the total\n",
    "    for word in question:\n",
    "        if word in vocabulary:\n",
    "            num_words = num_words + 1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    \n",
    "    try:\n",
    "        #Divide the result by the total number of words in the question to get the average\n",
    "        featureVec = np.divide(featureVec, num_words)\n",
    "    except RuntimeWarning:\n",
    "        print(featureVec)\n",
    "    return(featureVec)\n",
    "\n",
    "#Create average word vector for entire dataset\n",
    "def makeFeatureVec(questions, model, num_features):\n",
    "    count = 0\n",
    "    dataFeatureVec = np.zeros((len(questions), num_features), dtype = \"float32\")\n",
    "    \n",
    "    #Loop through each question in the dataset and calculate its average questions word vectors\n",
    "    for question in questions:\n",
    "        dataFeatureVec[count] = questionFeatureVec(question, model, num_features)\n",
    "        count += 1\n",
    "    return(dataFeatureVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the average word vectors for question1\n",
    "clean_question1 = []\n",
    "for question in data.question1:\n",
    "    clean_question1.append(question_to_wordlist(question, remove_stopwords = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:17: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "vecs_question1 = makeFeatureVec(clean_question1, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the average word vectors for question2\n",
    "clean_question2 = []\n",
    "for question in data.question2:\n",
    "    clean_question2.append(question_to_wordlist(question, remove_stopwords = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs_question2 = makeFeatureVec(clean_question2, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs_merge = [vecs_question1, vecs_question2]\n",
    "len(vecs_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(data['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit a Random Forest Classifier to the dataset - Using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest = forest.fit(vecs_merge, data['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import chardet\n",
    "# result = chardet.detect(data['question1'][11077])['encoding']\n",
    "# print(result)\n",
    "# print(data['question1'][11077].decode('Windows-1254'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Split the dataset into training and testing datasets\n",
    "##########################################\n",
    "#Loads the library required for splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Method 1 Features\n",
    "# features = data.ix[:,'len_q1':]\n",
    "\n",
    "#Method 2 Features\n",
    "features = diff_idf\n",
    "y = data.ix[:,'is_duplicate']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.2)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Random Forest Classifier\n",
    "##########################################\n",
    "#Loads required libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Create a Random Forest Classifer (clf by convention = 'classifier')\n",
    "clf = RandomForestClassifier(n_jobs=2) #n_jobs = # of jobs in run in parallel for fit and predict\n",
    "\n",
    "#Train the Random Forest Classifier\n",
    "clf.fit(X_train, y_train)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Apply Random Forest Classifer on the testing split of the dataset\n",
    "##########################################\n",
    "#Predicts the outcome variable of the testing split of the dataset\n",
    "test_prediction = clf.predict(X_test)\n",
    "\n",
    "#Prediction probability for the value of the outcome variable (0 or 1)\n",
    "test_prediction_proba = clf.predict_proba(X_test)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Apply Logloss function to Test Dataset Output\n",
    "##########################################\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss(np.array(y_test), test_prediction_proba)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preview newly added features to the dataset\n",
    "# pd.options.display.max_colwidth = 100\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Features of the training dataset\n",
    "# data.ix[:,'len_q1':].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data.ix[:,'len_q1':].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Outcome variable of the training dataset\n",
    "# data.ix[:,'is_duplicate'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data.ix[:,'is_duplicate'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preview Prediction\n",
    "# test_prediction[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preview the Prediction Probability [0, 1]\n",
    "# test_prediction_proba[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Displays Results in a Confusion Matrix\n",
    "#Anything on the diagonal was classified correctly and anything off the diagonal was classified incorrectly\n",
    "# pd.crosstab(y_test, test_prediction, rownames=['Actual Similarity'], colnames=['Predicted Similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Displays a list of features that were the most important in affecting the accuracy of the classification\n",
    "# important_features = list(zip(X_train, clf.feature_importances_))\n",
    "# important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Displays the accuracy score of the Random Forest Classifier on the test split\n",
    "# clf_accuracy = accuracy_score(y_test, test_prediction)\n",
    "# clf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Test Dataset\n",
    "##########################################\n",
    "#Test Dataset\n",
    "data_test = pd.read_csv('/Users/Priscilla/Desktop/QuoraDataset/test.csv')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Feature Engineering Method 1: \n",
    "# Creating new input variables to improve ML algorithm performance\n",
    "##########################################\n",
    "#Feature: Length of Question\n",
    "#Apply length function to every data.question1 & data.question2 observation\n",
    "data_test['len_q1'] = data_test.question1.apply(lambda x: len(str(x)))\n",
    "data_test['len_q2'] = data_test.question2.apply(lambda x: len(str(x)))\n",
    "\n",
    "#Feature: Difference in length between the Questions\n",
    "#Substract len_q1 from len_q2\n",
    "data_test['len_diff'] = data_test.len_q1 - data_test.len_q2\n",
    "\n",
    "#Feature: Character count of Question\n",
    "#Strip whitespace in data.question1 & data.question2 and apply the length function\n",
    "data_test['len_char_q1'] = data_test.question1.apply(lambda x: len(str(x).replace(' ', '')))\n",
    "data_test['len_char_q2'] = data_test.question2.apply(lambda x: len(str(x).replace(' ', '')))\n",
    "\n",
    "#Feature: Word count of Question\n",
    "#Call split function on every data.question1 & data.question2 observation and apply the length function\n",
    "data_test['len_word_q1'] = data_test.question1.apply(lambda x: len(str(x).split()))\n",
    "data_test['len_word_q2'] = data_test.question2.apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#Feature: Common words between the Questions\n",
    "#Intersection of data.question1 and data.question2\n",
    "#Set function is applied so repeated words in a question is omitted from the final common word count\n",
    "#Axis=1 to calculate the means column-wise (-->) rather than the default of Axis=0 to calculate the means row-wise(v)\n",
    "data_test['len_common_words'] = data_test.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Feature Engineering Method 2:\n",
    "# Create Bag Of Words Model with Tfidf Normalization\n",
    "##########################################\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Generate tfidf values for question1 and question2 based on the complete vocabulary of the dataset\n",
    "vectorizer_q1 = TfidfVectorizer(vocabulary = complete_vocab)\n",
    "vectorizer_q2 = TfidfVectorizer(vocabulary = complete_vocab)\n",
    "tfidf_question1 = vectorizer_q1.fit_transform(data_test.question1.values.astype(str))\n",
    "tfidf_question2 = vectorizer_q2.fit_transform(data_test.question2.values.astype(str))\n",
    "\n",
    "#Substract the difference of the tfidf weight matricies for the two questions\n",
    "#Will be 0 if the words are weighted the same in both questions (similar significance contribution)\n",
    "diff_idf = tfidf_question1 - tfidf_question2\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Apply Random Forest Classifer on the Test Dataset\n",
    "##########################################\n",
    "#Features of the test dataset\n",
    "#Method 1 Features\n",
    "#data_test_features = data_test.ix[:,'len_q1':]\n",
    "\n",
    "#Method 2 Features\n",
    "data_test_features = diff_idf\n",
    "\n",
    "#Predicts the outcome variable of the Test Dataset\n",
    "test_prediction = clf.predict(data_test_features)\n",
    "\n",
    "#Prediction probability for the value of the outcome variable (0 or 1)\n",
    "test_prediction_proba = clf.predict_proba(data_test_features)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Create Submission File\n",
    "##########################################\n",
    "submission = pd.DataFrame()\n",
    "submission['test_id'] = data_test.test_id\n",
    "submission['is_duplicate'] = test_prediction\n",
    "\n",
    "submission.to_csv('/Users/Priscilla/Desktop/QuoraDataset/submission.csv', index = False)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
