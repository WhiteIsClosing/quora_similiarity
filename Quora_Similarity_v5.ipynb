{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Load Required Python Libraries\n",
    "##########################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pylev import levenshtein\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import chardet\n",
    "import itertools\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from scipy.stats import kurtosis\n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Dataset\n",
    "##########################################\n",
    "#Training Dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "data['question1'] = data['question1'].astype(str)\n",
    "data['question2'] = data['question2'].astype(str)\n",
    "y = data['is_duplicate']\n",
    "df_train = data\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These are the functions for our transformers\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "model = KeyedVectors.load(\"300features_10minwords_5context\")\n",
    "\n",
    "def question_to_wordlist(text, remove_stopwords = False):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "def makeDistributionalFeatures(q1,q2):\n",
    "    data = pd.concat([q1, q2], axis=1)\n",
    "    features = []\n",
    "    \n",
    "    #For each question in the dataset:\n",
    "    # 1 - Compute similarity metric from word2vec model using every word combination between question1 and question2\n",
    "    # 2 - Create the distributional summary statistics for every combination of the disimilar words\n",
    "    for index in range(0, len(data)):\n",
    "            #Convert question1 and question2 into a list of words\n",
    "            question1 = question_to_wordlist(str(data.question1[index]))\n",
    "            question2 = question_to_wordlist(str(data.question2[index]))\n",
    "            \n",
    "            #Finds every word combination between question1 and question2\n",
    "            combinations = list(itertools.product(question1, question2))\n",
    "            combinations = [list(combination) for combination in combinations]\n",
    "            \n",
    "            #Tracks word2vec similarity metric for every word combination\n",
    "            values = []\n",
    "            \n",
    "            #Loops through each word combination\n",
    "            for combination in combinations:\n",
    "                #Checks if the model contains the words in its vocabulary\n",
    "                # 1 - Yes, adds it to the values list to calculate distributional stats with\n",
    "                # 2 - No, go to the next word pair\n",
    "                try:\n",
    "                    values.append(model.wv.similarity(combination[0], combination[1]))\n",
    "                except KeyError:\n",
    "                    pass\n",
    "      \n",
    "            #If there is at least one similarity metric calculate its mean and median\n",
    "            if(len(values) >= 1):\n",
    "                features.append([np.mean(values), np.median(values), np.std(values), kurtosis(values)])\n",
    "            else:\n",
    "                #Since we will not be deleting observations from the test dataset append [-1,-1,-1,-1] as stand in features\n",
    "                # 1 - The only combination contained a word our model does not contain\n",
    "                # 2 - Question1 or Question2 or both were \"\"\n",
    "                # 3 - We could not tokenize either Question1 or Question2\n",
    "                features.append([-1,-1,-1,-1])\n",
    "    return features\n",
    "\n",
    "def word_lengths(q1,q2):\n",
    "    data = pd.concat([q1, q2], axis=1)\n",
    "\n",
    "    #Feature: Length of Question\n",
    "    data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
    "    data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
    "\n",
    "    #Feature: Difference in length between the Questions\n",
    "    data['len_diff'] = data.len_q1 - data.len_q2\n",
    "\n",
    "    #Feature: Character count of Question\n",
    "    data['len_char_q1'] = data.question1.apply(lambda x: len(str(x).replace(' ', '')))\n",
    "    data['len_char_q2'] = data.question2.apply(lambda x: len(str(x).replace(' ', '')))\n",
    "\n",
    "    #Feature: Word count of Question\n",
    "    data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "    data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    #Feature: Common words between the Questions\n",
    "    data['len_common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "    return data.ix[:,'len_q1':'len_common_words']\n",
    "\n",
    "def shared_words(q1,q2):\n",
    "    question1_words = []\n",
    "    question2_words = []\n",
    "\n",
    "    for word in set(str(q1).lower().split()):\n",
    "        if word not in stop_words:\n",
    "            question1_words.append(word)\n",
    "\n",
    "    for word in set(str(q2).lower().split()):\n",
    "        if word not in stop_words:\n",
    "            question2_words.append(word)\n",
    "\n",
    "    #Question contains only stop words (or is an empty string)\n",
    "    if len(question1_words) == 0 or len(question2_words) == 0:\n",
    "        return 0\n",
    "\n",
    "    question1_shared_words = [w for w in question1_words if w in question2_words]\n",
    "    question2_shared_words = [w for w in question2_words if w in question1_words]\n",
    "\n",
    "    avg_words_shared = (len(question1_shared_words) + len(question2_shared_words))/(len(question1_words) + len(question2_words))\n",
    "    return avg_words_shared\n",
    "\n",
    "class LevDistanceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        \n",
    "        lev_distance_strings = [[a,b] \n",
    "        for a,b in zip(q1_list, q2_list)]\n",
    "        \n",
    "        lev_dist_array = np.array([\n",
    "    (float(levenshtein(pair[0], pair[1]))/\n",
    "    (float(sum([x.count('') for x in pair[0]])) + \n",
    "    float(sum([x.count('') for x in pair[1]])))) \n",
    "    for pair in lev_distance_strings \n",
    "        ])\n",
    "        \n",
    "        return lev_dist_array.reshape(len(lev_dist_array),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class TfIdfDiffTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self, total_words):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_questions = q1_list + q2_list\n",
    "        total_questions = [x for x in total_questions if type(x) != float]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words = 'english', vocabulary = total_words)\n",
    "        vectorizer.fit(total_questions)\n",
    "        tf_diff = vectorizer.transform(q1_list) - vectorizer.transform(q2_list)\n",
    "        return tf_diff\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class CosineDistTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_questions = q1_list + q2_list\n",
    "        total_questions = [x for x in total_questions if type(x) != float]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "        vectorizer.fit(total_questions)\n",
    "        \n",
    "        q1_tf = vectorizer.transform(q1_list) \n",
    "        q2_tf = vectorizer.transform(q2_list)\n",
    "        cos_sim = []\n",
    "        for i in range(0,len(q1_list)):\n",
    "            cos_sim.append(cosine_similarity(q1_tf[i], q2_tf[i])[0][0])\n",
    "            \n",
    "        return np.array(cos_sim).reshape(len(cos_sim),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class AverageSharedWords(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        avg_words = [shared_words(q1,q2) for q1, q2 in zip(q1_list, q2_list)]\n",
    "\n",
    "        return np.array(avg_words).reshape(len(avg_words),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class WordLengths(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        word_len = word_lengths(q1_list, q2_list)\n",
    "        return np.array(word_len)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self  \n",
    "    \n",
    "class Word2VecStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        stats = makeDistributionalFeatures(q1_list, q2_list)\n",
    "        return np.array(stats)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Combining all the features using FeatureUnion\n",
    "##########################################\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "# vectorizer.fit(df_train['question1'][0:5000] + df_train['question2'][0:5000])\n",
    "vectorizer.fit(df_train['question1'] + df_train['question2'])\n",
    "total_words = list(set(vectorizer.get_feature_names()))\n",
    "\n",
    "comb_features = FeatureUnion([('tf', TfIdfDiffTransformer(total_words)), \n",
    "                              ('cos_diff',CosineDistTransformer()), \n",
    "                              ('lev', LevDistanceTransformer()),\n",
    "                              ('AvgWords', AverageSharedWords()),\n",
    "                              ('WordLengths', WordLengths()),\n",
    "                              ('Word2VecStats', Word2VecStats())\n",
    "                             ])\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Split the dataset into training and testing datasets\n",
    "# ##########################################\n",
    "# y = df_train.ix[:,'is_duplicate'][0:5000]\n",
    "# all_features = comb_features.transform([df_train['question1'][0:5000], df_train['question2'][0:5000]])\n",
    "y = df_train.ix[:,'is_duplicate']\n",
    "all_features = comb_features.transform([df_train['question1'], df_train['question2']])\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, y, test_size=0.2, random_state=1317)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Running XGBoost\n",
    "##########################################\n",
    "# Set parameters for XGBoost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 8\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'test')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)\n",
    "joblib.dump(bst, 'xgboost_model_400iterations_8depth.pkl')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Test Dataset\n",
    "##########################################\n",
    "#Test Dataset\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#Replaces np.nan with ''\n",
    "df_test = df_test.replace(np.nan, '', regex=True)\n",
    "\n",
    "#Saves the cleaned test.csv\n",
    "df_test.to_csv('cleaned_test.csv')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Create the test features using FeatureUnion\n",
    "##########################################\n",
    "# test_features = comb_features.transform([df_test['question1'][0:5000], df_test['question2'][0:5000]])\n",
    "test_features = comb_features.transform([df_test['question1'], df_test['question2']])\n",
    "joblib.dump(test_features, 'test_features.pkl')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ensure Train and Test Features are the same size\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Predicting using XGBoost\n",
    "##########################################\n",
    "test = xgb.DMatrix(test_features)\n",
    "test_prediction = bst.predict(test)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Creating Submission File\n",
    "##########################################\n",
    "sub = pd.DataFrame()\n",
    "# sub['test_id'] = df_test['test_id'][0:5000]\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = test_prediction\n",
    "sub.to_csv('simple_xgb.csv', index=False)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check Submission File Length\n",
    "len(sub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
